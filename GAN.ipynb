{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution: Counter({5: 288, 6: 274, 8: 206, 2: 205, 0: 142, 7: 125, 1: 120, 3: 108, 4: 80})\n",
      "Images Needed Per Class: {0: 146, 1: 168, 2: 83, 3: 180, 4: 208, 5: 0, 6: 14, 7: 163, 8: 82}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "from collections import Counter\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = \"./images\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  \n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Get class distribution\n",
    "class_counts = Counter(dataset.targets)\n",
    "max_count = max(class_counts.values())  # Find max count to balance\n",
    "print(\"Class Distribution:\", class_counts)\n",
    "\n",
    "# Calculate images needed for each class\n",
    "images_needed = {cls: max_count - count for cls, count in class_counts.items()}\n",
    "print(\"Images Needed Per Class:\", images_needed)\n",
    "\n",
    "# Get class index mappings\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}  # Reverse mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, latent_dim=100):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 64 * 64 * 3),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, z):\n",
    "#         img = self.model(z)\n",
    "#         img = img.view(img.size(0), 3, 64, 64)\n",
    "#         return img\n",
    "    \n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(64 * 64 * 3, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         img = img.view(img.size(0), -1)  \n",
    "#         return self.model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize models\n",
    "# latent_dim = 100\n",
    "# generator = Generator(latent_dim).to(device)\n",
    "# discriminator = Discriminator().to(device)\n",
    "\n",
    "# # Optimizers & Loss\n",
    "# optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# criterion = nn.BCELoss()\n",
    "\n",
    "# # Train GAN\n",
    "# num_epochs = 5000\n",
    "# batch_size = 64\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     z = torch.randn(batch_size, latent_dim).to(device)\n",
    "#     fake_imgs = generator(z)\n",
    "\n",
    "#     # Train Discriminator\n",
    "#     real_imgs, _ = next(iter(dataloader))\n",
    "#     real_imgs = real_imgs.to(device)\n",
    "\n",
    "#     real_validity = discriminator(real_imgs)\n",
    "#     fake_validity = discriminator(fake_imgs.detach())\n",
    "\n",
    "#     real_loss = criterion(real_validity, torch.ones_like(real_validity))\n",
    "#     fake_loss = criterion(fake_validity, torch.zeros_like(fake_validity))\n",
    "#     d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "#     optimizer_D.zero_grad()\n",
    "#     d_loss.backward()\n",
    "#     optimizer_D.step()\n",
    "\n",
    "#     # Train Generator\n",
    "#     fake_validity = discriminator(fake_imgs)\n",
    "#     g_loss = criterion(fake_validity, torch.ones_like(fake_validity))\n",
    "\n",
    "#     optimizer_G.zero_grad()\n",
    "#     g_loss.backward()\n",
    "#     optimizer_G.step()\n",
    "\n",
    "#     if epoch % 500 == 0:\n",
    "#         print(f\"Epoch {epoch}: D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "epochs = 5000\n",
    "lr_g = 0.0002  # Lowered generator learning rate\n",
    "lr_d = 0.0001  # Lowered discriminator learning rate\n",
    "beta1 = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"./images\"\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, image_size * image_size * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.main(z)\n",
    "        img = img.view(-1, 3, image_size, image_size)\n",
    "        return img\n",
    "\n",
    "# Define Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(image_size * image_size * 3, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.main(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5000] | D Loss: 0.7576 | G Loss: 1.7919\n",
      "Epoch [500/5000] | D Loss: 0.8012 | G Loss: 1.6630\n",
      "Epoch [1000/5000] | D Loss: 0.6997 | G Loss: 2.0467\n",
      "Epoch [1500/5000] | D Loss: 0.7047 | G Loss: 2.1497\n",
      "Epoch [2000/5000] | D Loss: 0.6535 | G Loss: 2.0900\n",
      "Epoch [2500/5000] | D Loss: 0.6543 | G Loss: 2.0601\n",
      "Epoch [3000/5000] | D Loss: 0.6521 | G Loss: 2.0053\n",
      "Epoch [3500/5000] | D Loss: 0.6529 | G Loss: 2.0612\n",
      "Epoch [4000/5000] | D Loss: 0.7868 | G Loss: 1.8345\n",
      "Epoch [4500/5000] | D Loss: 0.8148 | G Loss: 1.6966\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Move data to device\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.shape[0]\n",
    "\n",
    "        # Generate noise\n",
    "        noise = torch.randn(batch_size, latent_dim, device=device)\n",
    "        fake_imgs = generator(noise)\n",
    "\n",
    "        # Create labels with smoothing\n",
    "        real_labels = torch.full((batch_size, 1), 0.9, device=device)  # Use 0.9 instead of 1\n",
    "        fake_labels = torch.full((batch_size, 1), 0.1, device=device)  # Use 0.1 instead of 0\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_imgs), real_labels)\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator twice as often\n",
    "        if i % 2 == 0:\n",
    "            optimizer_g.zero_grad()\n",
    "            g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "    # Print loss and save generated images\n",
    "    if epoch % 500== 0:\n",
    "        print(f\"Epoch [{epoch}/{epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "        vutils.save_image(fake_imgs.data[:25], f\"generated_{epoch}.png\", normalize=True)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 146 synthetic images saved for class 'chatamari'\n",
      "âœ… 168 synthetic images saved for class 'chhoila'\n",
      "âœ… 83 synthetic images saved for class 'dalbhat'\n",
      "âœ… 180 synthetic images saved for class 'dhindo'\n",
      "âœ… 208 synthetic images saved for class 'gundruk'\n",
      "âœ… 14 synthetic images saved for class 'momo'\n",
      "âœ… 163 synthetic images saved for class 'sekuwa'\n",
      "âœ… 83 synthetic images saved for class 'selroti'\n"
     ]
    }
   ],
   "source": [
    "# Generate images per class\n",
    "for class_label, num_images in images_needed.items():\n",
    "    if num_images <= 0:\n",
    "        continue  \n",
    "\n",
    "    save_path = os.path.join(dataset_path, idx_to_class[class_label])  \n",
    "    os.makedirs(save_path, exist_ok=True)  \n",
    "\n",
    "    z = torch.randn(num_images, latent_dim).to(device)\n",
    "    synthetic_imgs = generator(z) \n",
    "\n",
    "    for i, img in enumerate(synthetic_imgs):\n",
    "        vutils.save_image(img, f\"{save_path}/{idx_to_class[class_label]}_fake_{i}.png\")\n",
    "\n",
    "    print(f\"âœ… {num_images} synthetic images saved for class '{idx_to_class[class_label]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Final Balanced Class Distribution: Counter({'chatamari': 288, 'chhoila': 288, 'dalbhat': 288, 'dhindo': 288, 'gundruk': 288, 'kheer': 288, 'momo': 288, 'sekuwa': 288, 'selroti': 288})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Count final images\n",
    "final_counts = collections.Counter()\n",
    "for class_label in os.listdir(dataset_path):\n",
    "    class_dir = os.path.join(dataset_path, class_label)\n",
    "    if os.path.isdir(class_dir):\n",
    "        final_counts[class_label] = len(os.listdir(class_dir))\n",
    "\n",
    "print(\"ðŸ“Š Final Balanced Class Distribution:\", final_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
